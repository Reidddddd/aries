<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  ~ Copyright (c) 2019 R.C
  ~
  ~ Licensed under the Apache License, Version 2.0 (the "License");
  ~ you may not use this file except in compliance with the License.
  ~ You may obtain a copy of the License at
  ~
  ~     http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<!-- Put site-specific property overrides in this file. -->

<configuration>
  <!-- HDFS DataNode's configuration. -->

  <!--
    DataNode, General configurations
  -->
  <property>
    <name>dfs.datanode.data.dir</name><value>file://${hadoop.tmp.dir}/dfs/data</value>
  </property>
  <property>
    <name>dfs.datanode.hostname</name><value></value>
  </property>
  <property>
    <name>dfs.datanode.dns.interface</name><value>default</value>
  </property>
  <property>
    <name>dfs.datanode.dns.nameserver</name><value>defautl</value>
  </property>
  <property>
    <name>fs.trash.checkpoint.interval</name><value>0</value>
  </property>
  <property>
    <name>hadoop.rpc.socket.factory.class.default</name><value>org.apache.hadoop.net.StandardSocketFactory</value>
  </property>
  <property>
    <name>dfs.namenode.max-num-blocks-to-log</name><value>1000</value>
  </property>
  <property>
    <name>dfs.block.local-path-access.user</name><value></value>
  </property>
  <property>
    <name>dfs.datanode.use.datanode.hostname</name><value></value>
  </property>
  <property>
    <name>dfs.datanode.hdfs-blocks-metadata.enabled</name><value>false</value>
  </property>
  <property>
    <name>dfs.permissions.superusergroup</name><value>supergroup</value>
  </property>
  <property>
    <name>dfs.permissions.enabled</name><value>true</value>
  </property>
  <property>
    <name>dfs.pipeline.ecn</name><value>false</value>
  </property>
  <property>
    <name>dfs.datanode.network.counts.cache.max.size</name><value>2147483647</value><description>Integer.MAX_VALUE</description>
  </property>
  <property>
    <name>dfs.datanode.oob.timeout-ms</name><value>1500,0,0,0</value><description>???what is this</description>
  </property>
  <property>
    <name>dfs.client.socket-timeout</name><value>60000</value><description>milliseconds</description>
  </property>
  <property>
    <name>dfs.datanode.socket.write.timeout</name><value>480000</value><description>milliseconds</description>
  </property>
  <property>
    <name>dfs.datanode.socket.reuse.keepalive</name><value>4000</value>
  </property>
  <property>
    <name>dfs.datanode.transfer.socket.send.buffer.size</name><value>0</value>
  </property>
  <property>
    <name>dfs.datanode.transfer.socket.recv.buffer.size</name><value>0</value>
  </property>
  <property>
    <name>dfs.datanode.transferTo.allowed</name><value>true</value>
  </property>
  <property>
    <name>dfs.datanode.readahead.bytes</name><value>4194304</value><description>4MB</description>
  </property>
  <property>
    <name>dfs.datanode.drop.cache.behind.writes</name><value>false</value>
  </property>
  <property>
    <name>dfs.datanode.sync.behind.writes</name><value>false</value>
  </property>
  <property>
    <name>dfs.datanode.sync.behind.writes.in.background</name><value>false</value>
  </property>
  <property>
    <name>dfs.datanode.drop.cache.behind.reads</name><value>false</value>
  </property>


  <!--
    DataNode, Security related configuration: authentication, user identification
  -->
  <property>
    <name>hadoop.security.authentication</name>
    <value>simple</value>
  </property>
  <property>
    <name>hadoop.user.group.metrics.percentiles.intervals</name><value></value>
  </property>
  <!-- Kerberos related -->
  <property>
    <name>hadoop.security.auth_to_local</name><value></value>
  </property>
  <property>
    <name>hadoop.kerberos.min.seconds.before.relogin</name>
    <value>60</value>
    <description>seconds</description>
  </property>
  <property>
    <name>dfs.datanode.keytab.file</name><value></value>
  </property>
  <property>
    <name>dfs.datanode.kerberos.principal</name><value></value>
  </property>
  <!-- User Group Information -->
  <property>
    <name>hadoop.security.group.mapping</name>
    <value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value>
  </property>
  <property>
    <name>hadoop.security.groups.shell.command.timeout</name>
    <value>0</value>
    <description>1s,10s</description>
  </property>
  <property>
    <name>hadoop.security.groups.cache.secs</name>
    <value>300</value>
    <description>seconds</description>
  </property>
  <property>
    <name>hadoop.security.groups.negative-cache.secs</name>
    <value>30</value>
    <description>seconds</description>
  </property>
  <property>
    <name>hadoop.security.groups.cache.warn.after.ms</name>
    <value>5000</value>
    <description>milliseconds</description>
  </property>
  <property>
    <name>hadoop.security.groups.cache.background.reload</name>
    <value>false</value>
  </property>
  <property>
    <name>hadoop.security.groups.cache.background.reload.threads</name>
    <value>3</value>
  </property>
  <property>
    <name>hadoop.user.group.static.mapping.overrides</name>
    <value>dr.who=;</value>
  </property>


  <!--
    DataNode, StorageLocationChecker.
  -->
  <property>
    <name>dfs.datanode.disk.check.timeout</name><value>600000</value><description>10 mintues</description>
  </property>
  <property>
    <name>dfs.datanode.data.dir.perm</name><value>700</value>
  </property>
  <property>
    <name>dfs.datanode.failed.volumes.tolerated</name><value>0</value>
  </property>
  <property>
    <name>dfs.datanode.disk.check.min.gap</name><value>900000</value><description>15 minutes</description>
  </property>


  <!--
    DataNode, FileIoProvider
  -->
  <property>
    <name>dfs.datanode.fileio.profiling.sampling.percentage</name><value>0</value>
  </property>
  <property>
    <name>dfs.datanode.enable.fileio.fault.injection</name><value>false</value>
  </property>


  <!--
    DataNode, BlockScanner
  -->
  <property>
    <name>dfs.block.scanner.volume.bytes.per.second</name><value>1048576</value><description>1MB</description>
  </property>
  <property>
    <name>internal.dfs.block.scanner.max_staleness.ms</name><value>15</value><description>minutes</description>
  </property>
  <property>
    <name>internal.dfs.datanode.scan.period.ms.key</name><value>${dfs.datanode.scan.period.hours}</value>
  </property>
  <property>
    <name>dfs.datanode.scan.period.hours</name><value>21*24</value><description>3 weeks</description>
  </property>
  <property>
    <name>dfs.block.scanner.cursor.save.interval.ms</name><value>10</value><description>minutes</description>
  </property>
  <property>
    <name>internal.volume.scanner.scan.result.handler</name><value>org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler</value>
  </property>


  <!--
    DataNode, DatasetVolumeChecker. All its configurations are exactly the same as StorageLocationChecker
  -->


  <!--
    DataNode, Shortcircuit related
  -->
  <property>
    <name>dfs.client.read.shortcircuit</name><value>false</value>
  </property>
  <property>
    <name>dfs.client.read.shortcircuit.skip.checksum</name><value>false</value>
  </property>
  <property>
    <name>dfs.client.read.shortcircuit.streams.cache.size</name><value>256</value>
  </property>
  <property>
    <name>dfs.client.read.shortcircuit.streams.cache.expiry.ms</name><value>300000</value>
  </property>


  <property>
    <name>ipc.server.read.connection-queue.size</name>
    <value>100</value>
  </property>
  <property>
    <name>hadoop.security.authorization</name>
    <value>false</value>
  </property>
  <property>
    <name>ipc.server.tcpnodelay</name>
    <value>true</value>
  </property>
  <property>
    <name>ipc.server.log.slow.rpc</name>
    <value>false</value>
  </property>
  <!-- Differente RPC Service(Client|Server|Lifeline) is dintigushed by port number -->
  <property>
    <name>ipc.${service_port}.callqueue.impl</name>
    <value>java.util.concurrent.LinkedBlockingQueue</value>
  </property>
  <property>
    <name>ipc.${service_port}.scheduler.impl</name>
    <value>org.apache.hadoop.ipc.DefaultRpcScheduler</value>
  </property>
  <property>
    <name>ipc.${service_port}.backoff.enable</name>
    <value>false</value>
  </property>
  <property>
    <name>ipc.${service_port}.faircallqueue.priority-levels</name>
    <value>0</value>
  </property>
  <property>
    <name>ipc.${service_port}.scheduler.priority.levels</name>
    <value>4</value>
  </property>
  <!-- Listener -->
  <property>
    <name>ipc.server.listen.queue.size</name>
    <value>128</value>
  </property>
  <!-- ConnectionManager -->
  <property>
    <name>ipc.client.idlethreshold</name>
    <value>4000</value>
  </property>
  <property>
    <name>ipc.client.connection.idle-scan-interval.ms</name>
    <value>10000</value>
  </property>
  <property>
    <name>ipc.client.connection.maxidletime</name>
    <value>10000</value>
  </property>
  <property>
    <name>ipc.client.kill.max</name>
    <value>10</value>
  </property>
  <property>
    <name>ipc.server.max.connections</name>
    <value>0</value>
  </property>
  <!-- RPC Metric -->
  <property>
    <name>rpc.metrics.percentiles.intervals</name><value></value>
  </property>
  <property>
    <name>rpc.metrics.quantile.enable</name>
    <value>false</value>
  </property>
  <!-- SASL -->
  <property>
    <name>hadoop.security.saslproperties.resolver.class</name>
    <value>org.apache.hadoop.security.SaslPropertiesResolver</value>
  </property>
  <property>
    <name>hadoop.rpc.protection</name>
    <value>auth</value>
    <description>auth-int,auth-conf</description>
  </property>


  <!--
    NameNode.JVM Monitor
  -->
  <property>
    <name>jvm.pause.warn-threshold.ms</name>
    <value>10000</value>
    <description>milliseconds</description>
  </property>
  <property>
    <name>jvm.pause.info-threshold.ms</name>
    <value>1000</value>
    <description>milliseconds</description>
  </property>


  <!--
    NameNode.FSNamesystem
  -->
  <!-- KMS, related to encryption zone, temporary skip -->
  <property>
    <name>hadoop.security.key.provider.path</name><value></value>
  </property>
  <property>
    <name>hadoop.ssl.require.client.cert</name>
    <value>false</value>
  </property>
  <property>
    <name>hadoop.ssl.keystores.factory.class</name>
    <value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value>
  </property>
  <property>
    <name>hadoop.ssl.enabled.protocols</name>
    <value>TLSv1,SSLv2Hello,TLSv1.1,TLSv1.2</value>
  </property>
  <property>
    <name>hadoop.ssl.client.conf</name>
    <value>ssl-client.xml</value>
  </property>
  <property>
    <name>hadoop.ssl.server.conf</name>
    <value>ssl-server.xml</value>
  </property>
  <property>
    <name>ssl.server.exclude.cipher.list</name><value></value>
  </property>
  <!-- FSServerDefault, some sever side default configurations, but can be overrided by client side -->
  <property>
    <name>io.file.buffer.size</name>
    <value>4096</value>
    <description>4KB</description>
  </property>
  <property>
    <name>fs.trash.interval</name>
    <value>0</value>
  </property>
  <!-- BlockManager -->
  <property>
    <name>net.topology.impl</name><value>org.apache.hadoop.net.NetworkTopology</value>
  </property>
  <property>
    <name>net.topology.node.switch.mapping.impl</name><value>org.apache.hadoop.net.ScriptBasedMapping</value>
  </property>
  <property>
    <name>net.topology.script.file.name</name><value></value>
  </property>
  <property>
    <name>net.topology.script.number.args</name><value>100</value>
  </property>



</configuration>
