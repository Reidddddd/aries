<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  ~ Copyright (c) 2019 R.C
  ~
  ~ Licensed under the Apache License, Version 2.0 (the "License");
  ~ you may not use this file except in compliance with the License.
  ~ You may obtain a copy of the License at
  ~
  ~     http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<!-- Put site-specific property overrides in this file. -->

<configuration>
  <!-- HDFS NameNode's configuration. -->

  <!--
    NameNode, NameNodeHttpServer. Configurations are all about the web UI.
  -->
  <!-- About HttpServer -->
  <property>
    <name>hadoop.http.authentication.type</name>
    <value>simple</value>
  </property>
  <property>
    <name>hadoop.http.authentication.token.validity</name>
    <value>36000</value>
  </property>
  <property>
    <name>hadoop.http.authentication.signature.secret.file</name>
    <value>${user.home}/hadoop-http-auth-signature-secret</value>
  </property>
  <property>
    <name>hadoop.http.authentication.cookie.domain</name><value></value>
  </property>
  <property>
    <name>hadoop.http.authentication.simple.anonymous.allowed</name>
    <value>true</value>
  </property>
  <property>
    <name>hadoop.http.authentication.kerberos.principal</name>
    <value>HTTP/_HOST@LOCALHOST</value>
  </property>
  <property>
    <name>hadoop.http.authentication.kerberos.keytab</name>
    <value>${user.home}/hadoop.keytab</value>
  </property>
  <property>
    <name>hadoop.http.max.threads</name>
    <value>-1</value>
  </property>
  <property>
    <name>hadoop.http.logs.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>hadoop.jetty.logs.serve.aliases</name>
    <value>true</value>
  </property>
  <property>
    <name>hadoop.http.filter.initializers</name>
    <value>org.apache.hadoop.http.lib.StaticUserWebFilter</value>
  </property>
  <property>
    <name>hadoop.http.staticuser.user</name>
    <value>dr.who</value>
  </property>


  <!--
    NameNode.NameNodeRpcServer. Configurations are about processing request.
  -->
  <property>
    <name>ipc.maximum.data.length</name>
    <value>67108864</value>
    <description>64MB</description>
  </property>
  <property>
    <name>ipc.server.handler.queue.size</name>
    <value>100</value>
  </property>
  <property>
    <name>ipc.server.max.response.size</name>
    <value>1048576</value>
    <description>1MB</description>
  </property>
  <property>
    <name>ipc.server.read.threadpool.size</name>
    <value>1</value>
  </property>
  <property>
    <name>ipc.server.read.connection-queue.size</name>
    <value>100</value>
  </property>
  <property>
    <name>ipc.${service_port}.callqueue.impl</name>
    <value>java.util.concurrent.LinkedBlockingQueue</value>
  </property>
  <property>
    <name>ipc.${service_port}.scheduler.impl</name>
    <value>org.apache.hadoop.ipc.DefaultRpcScheduler</value>
  </property>
  <property>
    <name>ipc.${service_port}.backoff.enable</name>
    <value>false</value>
  </property>
  <property>
    <name>ipc.${service_port}.faircallqueue.priority-levels</name>
    <value>0</value>
  </property>
  <property>
    <name>ipc.${service_port}.scheduler.priority.levels</name>
    <value>4</value>
  </property>
  <property>
    <name>hadoop.security.authorization</name>
    <value>false</value>
  </property>
  <property>
    <name>hadoop.security.authentication</name>
    <value>simple</value>
  </property>
  <property>
    <name>ipc.server.listen.queue.size</name>
    <value>128</value>
  </property>
  <property>
    <name>ipc.client.idlethreshold</name>
    <value>4000</value>
  </property>
  <property>
    <name>ipc.client.connection.idle-scan-interval.ms</name>
    <value>10000</value>
  </property>
  <property>
    <name>ipc.client.connection.maxidletime</name>
    <value>10000</value>
  </property>
  <property>
    <name>ipc.client.kill.max</name>
    <value>10</value>
  </property>
  <property>
    <name>ipc.server.max.connections</name>
    <value>0</value>
  </property>
  <property>
    <name>ipc.server.tcpnodelay</name>
    <value>true</value>
  </property>
  <property>
    <name>ipc.server.log.slow.rpc</name>
    <value>false</value>
  </property>
  <property>
    <name>hadoop.security.saslproperties.resolver.class</name>
    <value>org.apache.hadoop.security.SaslPropertiesResolver</value>
  </property>
  <property>
    <name>hadoop.rpc.protection</name>
    <value>auth</value>
    <description>auth-int,auth-conf</description>
  </property>
  <property>
    <name>rpc.metrics.percentiles.intervals</name><value></value>
  </property>
  <property>
    <name>rpc.metrics.quantile.enable</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.namenode.min.supported.datanode.version</name>
    <value>2.1.0-beta</value>
  </property>
  <property>
    <name></name>
  </property>

</configuration>
