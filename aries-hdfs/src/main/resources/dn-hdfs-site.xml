<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  ~ Copyright (c) 2019 R.C
  ~
  ~ Licensed under the Apache License, Version 2.0 (the "License");
  ~ you may not use this file except in compliance with the License.
  ~ You may obtain a copy of the License at
  ~
  ~     http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<!-- Put site-specific property overrides in this file. -->

<configuration>
  <!-- HDFS DataNode's configuration. -->


  <!--
    DataNode, General configurations
  -->
  <property>
    <name>dfs.datanode.data.dir</name><value>file://${hadoop.tmp.dir}/dfs/data</value>
  </property>
  <property>
    <name>dfs.datanode.transferTo.allowed</name><value>true</value>
  </property>
  <property>
    <name>dfs.datanode.readahead.bytes</name><value>4194304</value><description>4MB</description>
  </property>
  <property>
    <name>dfs.datanode.drop.cache.behind.reads</name><value>false</value>
  </property>
  <property>
    <name>dfs.datanode.outliers.report.interval</name><value>1800000</value>
  </property>
  <property>
    <name>dfs.datanode.slow.io.warning.threshold.ms</name><value>300</value>
  </property>
  <property>
    <name>dfs.datanode.max.locked.memory</name><value>0</value>
  </property>


  <!--
    DataNode, Security related configuration: authentication, user identification
  -->
  <property>
    <name>dfs.datanode.keytab.file</name><value></value>
  </property>
  <property>
    <name>dfs.datanode.kerberos.principal</name><value></value>
  </property>


  <!--
    DataNode, StorageLocationChecker && DatasetVolumeChecker
  -->
  <property>
    <name>dfs.datanode.disk.check.timeout</name><value>600000</value><description>10 mintues</description>
  </property>
  <property>
    <name>dfs.datanode.data.dir.perm</name><value>700</value> <!-- DatasetVolumeChecker doesn't use this one-->
  </property>
  <property>
    <name>dfs.datanode.failed.volumes.tolerated</name><value>0</value>
  </property>
  <property>
    <name>dfs.datanode.disk.check.min.gap</name><value>900000</value><description>15 minutes</description>
  </property>


  <!--
    DataNode, FileIoProvider
  -->
  <property>
    <name>dfs.datanode.fileio.profiling.sampling.percentage</name><value>0</value> <!-- Disk Metrics -->
  </property>
  <property>
    <name>dfs.datanode.enable.fileio.fault.injection</name><value>false</value>
  </property>


  <!--
    DataNode, BlockScanner
  -->
  <property>
    <name>dfs.block.scanner.volume.bytes.per.second</name><value>1048576</value><description>1MB</description>
  </property>
  <property>
    <name>internal.dfs.block.scanner.max_staleness.ms</name><value>15</value><description>minutes</description>
  </property>
  <property>
    <name>internal.dfs.datanode.scan.period.ms.key</name><value>${dfs.datanode.scan.period.hours}</value>
  </property>
  <property>
    <name>dfs.datanode.scan.period.hours</name><value>21*24</value><description>3 weeks</description>
  </property>
  <property>
    <name>dfs.block.scanner.cursor.save.interval.ms</name><value>10</value><description>minutes</description>
  </property>
  <property>
    <name>internal.volume.scanner.scan.result.handler</name><value>org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler</value>
  </property>


  <!--
    DataNode, DataXceiver
  -->
  <property>
    <name>dfs.datanode.max.transfer.threads</name><value>4096</value>
  </property>
  <property>
    <name>dfs.blocksize</name><value>134217728</value><description>128MB</description>
  </property>
  <property>
    <name>dfs.datanode.balance.bandwidthPerSec</name><value>10M</value>
  </property>
  <property>
    <name>dfs.datanode.balance.max.concurrent.moves</name><value>50</value>
  </property>
  <property>
    <name>dfs.datanode.socket.reuse.keepalive</name><value>4000</value>
  </property>
  <property>
    <name>dfs.datanode.transfer.socket.send.buffer.size</name><value>0</value>
  </property>
  <property>
    <name>dfs.datanode.drop.cache.behind.writes</name><value>false</value>
  </property>
  <property>
    <name>dfs.datanode.sync.behind.writes</name><value>false</value>
  </property>
  <property>
    <name>dfs.datanode.sync.behind.writes.in.background</name><value>false</value>
  </property>
  <property>
    <name>dfs.datanode.synconclose</name><value>false</value>
  </property>
  <property>
    <name>dfs.datanode.xceiver.stop.timeout.millis</name><value>60000</value>
  </property>
  <property>
    <name>dfs.datanode.restart.replica.expiration</name><value>50</value><description>seconds</description>
  </property>
  <property>
    <name>dfs.datanode.non.local.lazy.persist</name><value>false</value>
  </property>
  <property>
    <name>dfs.datanode.bp-ready.timeout</name><value>20</value>
  </property>
  <!-- TCPPeerServer -->
  <property>
    <name>dfs.datanode.socket.write.timeout</name><value>480000</value><description>milliseconds</description>
  </property>
  <property>
    <name>dfs.datanode.address</name><value>0.0.0.0:50010</value>
  </property>
  <property>
    <name>dfs.datanode.transfer.socket.recv.buffer.size</name><value>0</value> <!-- DomainPeerServer also use this one -->
  </property>
  <!-- DomainPeerServer -->
  <property>
    <name>dfs.client.read.shortcircuit</name><value>false</value>
  </property>
  <property>
    <name>dfs.client.domain.socket.data.traffic</name><value>false</value>
  </property>
  <property>
    <name>dfs.domain.socket.path</name><value></value>
  </property>
  <!-- ShortCircuitRegistry -->
  <property>
    <name>dfs.short.circuit.shared.memory.watcher.interrupt.check.ms</name><value>60000</value>
  </property>
  <property>
    <name>dfs.datanode.shared.file.descriptor.paths</name><value>/dev/shm,/tmp</value>
  </property>


  <!--
    DataNode, InfoServer
  -->
  <property>
    <name>dfs.webhdfs.rest-csrf.enabled</name><value>false</value>
  </property>
  <property>
    <name>dfs.webhdfs.rest-csrf.custom-header</name><value>X-XSRF-HEADER</value>
  </property>
  <property>
    <name>dfs.webhdfs.rest-csrf.methods-to-ignore</name><value>GET,OPTIONS,HEAD,TRACE</value>
  </property>
  <property>
    <name>dfs.webhdfs.rest-csrf.browser-useragents-regex</name><value>^Mozilla.*,^Opera.*</value>
  </property>
  <property>
    <name>dfs.datanode.http.max-threads</name><value>10</value>
  </property>
  <property>
    <name>dfs.datanode.http.internal-proxy.port</name><value>0</value>
  </property>
  <property>
    <name>dfs.cluster.administrators</name><value></value>
  </property>
  <property>
    <name>dfs.datanode.http.address</name><value>0.0.0.0:50075</value>
  </property>
  <property>
    <name>dfs.datanode.https.address</name><value>0.0.0.0:50475</value>
  </property>
  <property>
    <name>dfs.xframe.enabled</name><value>true</value>
  </property>
  <property>
    <name>dfs.xframe.value</name><value>SAMEORIGIN</value>
  </property>
  <property>
    <name>dfs.webhdfs.ugi.expire.after.access</name><value>600000</value><description>10 mins</description>
  </property>
  <property>
    <name>dfs.http.policy</name><value>HTTP_ONLY</value><description>HTTP_AND_HTTPS,HTTPS_ONLY</description>
  </property>
  <property>
    <name>dfs.https.enable</name><value>false</value>
  </property>
  <property>
    <name>dfs.webhdfs.user.provider.user.pattern</name><value>^[A-Za-z_][A-Za-z0-9._-]*[$]?$</value>
  </property>
  <property>
    <name>dfs.webhdfs.acl.provider.permission.pattern</name><value>^(default:)?(user|group|mask|other):[[A-Za-z_][A-Za-z0-9._-]]*:([rwx-]{3})?(,(default:)?(user|group|mask|other):[[A-Za-z_][A-Za-z0-9._-]]*:([rwx-]{3})?)*$</value>
  </property>
  <property>
    <name>dfs.webhdfs.netty.high.watermark</name><value>65535</value>
  </property>
  <property>
    <name>dfs.webhdfs.netty.high.watermark</name><value>65535</value>
  </property>


  <!--
    DataNode, IPC/RPC Server
  -->
  <property>
    <name>dfs.datanode.ipc.address</name><value>0.0.0.0:50020</value>
  </property>
  <property>
    <name>dfs.datanode.handler.count</name><value>10</value>
  </property>


  <!--
    DataNode, Metrics
  -->
  <property>
    <name>dfs.metrics.percentiles.intervals</name><value></value>
  </property>
  <property>
    <name>dfs.metrics.session-id</name><value></value>
  </property>
  <property>
    <name>dfs.datanode.peer.stats.enabled</name><value>false</value>
  </property>
  <property>
    <name>dfs.datanode.metrics.logger.period.seconds</name><value>600</value>
  </property>


  <!--
    DataNode, BlockRecoveryWorker
  -->
  <property>
    <name>dfs.client.socket-timeout</name><value>60000</value><description>milliseconds</description>
  </property>
  <property>
    <name>dfs.datanode.use.datanode.hostname</name><value>false</value>
  </property>


  <!--
    DataNode, BlockPoolManager
  -->
  <property>
    <name>dfs.nameservices</name><value></value>
  </property>
  <property>
    <name>dfs.namenode.servicerpc-address</name><value></value>
  </property>
  <property>
    <name>dfs.namenode.servicerpc-bind-host</name><value></value>
  </property>
  <property>
    <name>dfs.blockreport.intervalMsec</name><value>2160000</value><description>6 hours</description>
  </property>
  <property>
    <name>dfs.blockreport.incremental.intervalMsec</name><value>0</value>
  </property>
  <property>
    <name>dfs.blockreport.split.threshold</name><value>100000</value>
  </property>
  <property>
    <name>dfs.cachereport.intervalMsec</name><value>10000</value>
  </property>
  <property>
    <name>dfs.blockreport.initialDelay</name><value>0</value>
  </property>
  <property>
    <name>dfs.heartbeat.interval</name><value>3</value>
  </property>
  <property>
    <name>dfs.datanode.lifeline.interval.seconds</name><value>9</value>
  </property>
  <property>
    <name>dfs.datanode.min.supported.namenode.version</name><value>2.1.0-beta</value>
  </property>


  <!--
    DataNode, SASLDataTransfer
  -->
  <property>
    <name>dfs.data.transfer.protection</name><value></value>
  </property>
  <property>
    <name>hadoop.security.saslproperties.resolver.class</name><value>org.apache.hadoop.security.SaslPropertiesResolver</value>
  </property>
  <property>
    <name>dfs.data.transfer.saslproperties.resolver.class</name><value>${hadoop.security.saslproperties.resolver.class}</value>
  </property>
  <property>
    <name>dfs.trustedchannel.resolver.class</name><value>org.apache.hadoop.hdfs.protocol.datatransfer.TrustedChannelResolver</value>
  </property>
  <property>
    <name>dfs.encrypt.data.transfer.cipher.suites</name><value></value>
  </property>
  <property>
    <name>hadoop.security.crypto.codec.classes.aes.ctr.nopadding</name><value>org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec</value>
  </property>
  <property>
    <name>dfs.encrypt.data.transfer</name><value>false</value>
  </property>
  <property>
    <name>ignore.secure.ports.for.testing</name><value>false</value>
  </property>
  <property>
    <name>dfs.encrypt.data.transfer.algorithm</name><value></value>
  </property>


  <!--
    DataNode, FsDatasetImpl
  -->
  <property>
    <name>dfs.lock.suppress.warning.interval</name><value>10000</value><description>10s</description>
  </property>
  <property>
    <name>dfs.datanode.ram.disk.replica.tracker</name><value>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker</value>
  </property>
  <property>
    <name>dfs.datanode.fsdataset.volume.choosing.policy</name><value>org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy</value>
  </property>
  <property>
    <name>dfs.datanode.lazywriter.interval.sec</name><value>60</value>
  </property>
  <property>
    <name>dfs.datanode.block-pinning.enabled</name><value>false</value>
  </property>
  <property>
    <name>fs.df.interval</name><value>60000</value>
  </property>
  <property>
    <name>dfs.datanode.du.reserved.calculator</name><value>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorAbsolute</value>
  </property>
  <property>
    <name>dfs.datanode.du.reserved</name><value>0</value>
  </property>
  <property>
    <name>dfs.datanode.du.reserved.pct</name><value>0</value>
  </property>
  <property>
    <name>dfs.datanode.duplicate.replica.deletion</name><value>false</value>
  </property>
  <property>
    <name>dfs.datanode.cached-dfsused.check.interval.ms</name><value>600000</value>
  </property>
  <property>
    <name>dfs.support.append</name><value>true</value>
  </property>


</configuration>
